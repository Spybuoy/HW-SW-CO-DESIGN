{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dhwzJTzYoO0"
      },
      "outputs": [],
      "source": [
        "# # ESE 5390 Lab 4\n",
        "\n",
        "# This lab will walk through an example of pruning a model to reduce the size of the network while mitigating accuracy loss.\n",
        "# The benefits of pruning include lower storage costs and possibly lower computation cost for optimized architectures.\n",
        "\n",
        "# This lab makes heavy use of prior labs and tutorials. Please make sure to review these prior labs and tutorials while completing this lab. \n",
        "\n",
        "# ## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbV5QrTEYoO1",
        "outputId": "cd261aed-61bf-4b7c-e316-d62cdb8a736a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'data': No such file or directory\n",
            "PyTorch Version:  1.12.1+cu113\n",
            "Torchvision Version:  0.13.1+cu113\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch.nn.utils.prune as prune\n",
        "from torchvision import datasets, models, transforms\n",
        "from pathlib import Path\n",
        "import os\n",
        "import time\n",
        "!rm data || rm -rf data\n",
        "!mkdir data\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"Torchvision Version: \", torchvision.__version__)\n",
        "import gc\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_to_use = 'cuda'"
      ],
      "metadata": {
        "id": "w9zt1pDiKWM9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e76CkayhYoO2",
        "outputId": "c2c11d3f-29cc-4b69-e23e-1032712f90fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./drive\n",
            "Finished copying from Drive to local.\n"
          ]
        }
      ],
      "source": [
        "# Mount data directory to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('./drive')\n",
        "\n",
        "# Copy to local for faster extraction\n",
        "!cp drive/Shareddrives/penn-ese-5390-202230/ILSVRC2012_img_val.tar ./data\n",
        "!cp drive/Shareddrives/penn-ese-5390-202230/ILSVRC2012_devkit_t3.tar.gz ./data\n",
        "!cp drive/Shareddrives/penn-ese-5390-202230/ILSVRC2012_devkit_t12.tar.gz ./data\n",
        "\n",
        "# drive.mount('./drive', force_remount=True)\n",
        "# !cp /content/drive/Shareddrives/539/ILSVRC2012_img_val.tar ./data\n",
        "\n",
        "# !cp /content/drive/Shareddrives/539/ILSVRC2012_devkit_t3.tar.gz ./data\n",
        "\n",
        "# !cp /content/drive/Shareddrives/539/ILSVRC2012_devkit_t12.tar.gz ./data\n",
        "print('Finished copying from Drive to local.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjbVQ8h-YoO2",
        "outputId": "edc22d29-d8de-4184-cee3-ce68b4267a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of validation images: 50000\n"
          ]
        }
      ],
      "source": [
        "# Create transform to preprocess data\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create validation dataset\n",
        "val_dataset = datasets.ImageNet('./data', split='val', transform=val_transform)\n",
        "\n",
        "# Create validation dataloader\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f'Number of validation images: {len(val_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ks3DaFKpYoO3"
      },
      "outputs": [],
      "source": [
        "# Put your reusable functions here.\n",
        "# You can copy functions from previous labs and tutorials.\n",
        "def validate_model(model, dataloader, n=1024, device = 'cpu'):\n",
        "      model.eval()\n",
        "      running_corrects = 0\n",
        "      total = 0\n",
        "\n",
        "      # Move model to device (CPU or GPU)\n",
        "      # One time to amortize data movement\n",
        "      dev = torch.device(device)\n",
        "      model.to(dev)\n",
        "\n",
        "      # Iterate over data stopping early if n is set\n",
        "      t1 = time.time()\n",
        "      for i, (inputs, labels) in enumerate(dataloader):\n",
        "          if (n is not None and i >= n):\n",
        "              break\n",
        "          # Send inputs to device\n",
        "          inputs = inputs.to(dev)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          \n",
        "          \"\"\"\n",
        "          \"\"\"\n",
        "          temp = labels.data\n",
        "          if(device==\"cuda\"):\n",
        "            temp = temp.to(device=\"cuda\")\n",
        "\n",
        "\n",
        "          # Gather statistics\n",
        "          running_corrects += torch.sum(predicted == temp)\n",
        "          total += inputs.size()[0]       # get batch size\n",
        "\n",
        "          if i % 200 == 199:\n",
        "              acc = 100 * running_corrects.double() / total\n",
        "              print(f'[{i + 1}] {acc:.4f}%')\n",
        "      t2 = time.time()\n",
        "      exe_time = t2-t1\n",
        "      epoch_acc = 100 * running_corrects.double() / total\n",
        "      del model\n",
        "      del inputs\n",
        "      gc.collect()\n",
        "      \n",
        "      torch.cuda.empty_cache()\n",
        "      return epoch_acc, exe_time\n",
        "def get_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    model_size = os.path.getsize(\"temp.p\")/1e6\n",
        "    os.remove('temp.p')\n",
        "    return model_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiFF_eEMYoO3"
      },
      "source": [
        "## Question 1: Global Pruning\n",
        "\n",
        "- Perform pruning using `prune.global_unstructured` on a pretrained VGG16 using various sparsity targets.\n",
        "- The models should be pruned away at 5% increments from 5% to 45% total pruning of the model.\n",
        "- Save a copy of each pruned model in a dictionary.\n",
        "- Plot the top-1 accuracy against weight pruned from 0% to 45% (use the original VGG16 for 0%)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orig_model = models.vgg16(pretrained = True)#TODO: load pretrained model\n",
        "orig_model.eval()\n",
        "orig_acc, orig_time = validate_model(orig_model, val_dataloader, 1024, device = device_to_use)\n",
        "print(f'Validation Finished. Original Time: {orig_time:.1f}s. Original Accuracy: {orig_acc:.1f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "ac27ca66b58b4d9db7ea3408426a5f1d",
            "ac61a95d99ac4592b53f2cc602535455",
            "a885235ea97b4e8ea070f3aa8d7242b6",
            "595f0041c48140d9b4f2ca985d6afda2",
            "903b1a73edb443c4ba0d85c0d51b6fc8",
            "72d95cd2a2604b89839024fff046179b",
            "1808c50ae4074b37b9af5c89d74fc4e3",
            "161812323f504913a4d350365dcc0b43",
            "e4b95eb0084749979e01eac603d4a22c",
            "4c2db2c1cbcd4276a33bf047ec14a3f6",
            "7afd7ffbdee041f187c88dd96e2469dd"
          ]
        },
        "id": "hq6lyjrpF0VQ",
        "outputId": "518565df-c1f9-4846-87e7-82a20cbbe4ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac27ca66b58b4d9db7ea3408426a5f1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200] 85.5000%\n",
            "[400] 82.0000%\n",
            "[600] 84.8333%\n",
            "[800] 87.0000%\n",
            "[1000] 87.4000%\n",
            "Validation Finished. Original Time: 22.3s. Original Accuracy: 87.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prune_rate_list = range(5, 50, 5) # List of prune rate to test from 5-45 inclusive with step of 5\n",
        "global_pruning = {} # Dictionary to store global pruning results\n",
        "for prune_rate in prune_rate_list:\n",
        "    model = orig_model #TODO: load pretrained model\n",
        "    parameter_to_prune = (\n",
        "        (model.features[0], 'weight'), # conv1 of VGG16\n",
        "        (model.features[2], 'weight'),\n",
        "        (model.features[5], 'weight'),\n",
        "        (model.features[7], 'weight'),\n",
        "        (model.features[10], 'weight'),\n",
        "        (model.features[12], 'weight'),\n",
        "        (model.features[14], 'weight'),\n",
        "        (model.features[17], 'weight'),\n",
        "        (model.features[19], 'weight'),\n",
        "        (model.features[21], 'weight'),\n",
        "        (model.features[24], 'weight'),\n",
        "        (model.features[26], 'weight'),\n",
        "        (model.features[28], 'weight'),\n",
        "        (model.classifier[0], 'weight'),\n",
        "        (model.classifier[3], 'weight'),\n",
        "        (model.classifier[6], 'weight')\n",
        "        #TODO: Add more layers to prune\n",
        "    )\n",
        "    #TODO: Prune model\n",
        "    prune.global_unstructured(parameter_to_prune,pruning_method=prune.L1Unstructured,amount=prune_rate/100.0,)\n",
        "    #TODO: make the pruning permanent to increase speed\n",
        "    for param, name in parameter_to_prune:\n",
        "      prune.remove(param, name)\n",
        "    global_pruning[prune_rate] = {} # Dictionary to store accuracy results and model for each prune rate\n",
        "    global_pruning[prune_rate]['model'] = model.state_dict() # Copy pruned model to dictionary\n",
        "    # TODO Run validation on the pruned model\n",
        "    pruned_acc, pruned_time = validate_model(model, val_dataloader, 1024, device = device_to_use)\n",
        "    del model\n",
        "    torch.cuda.clear_cache\n",
        "    print(f'Validation Finished. Pruned Time: {pruned_time:.1f}s. Pruned Accuracy: {pruned_acc:.1f}%')\n",
        "    print(f'Speedup: {orig_time/pruned_time}x. Accuracy drop: {orig_acc-pruned_acc}%')\n",
        "    global_pruning[prune_rate]['top1_acc'] = pruned_acc # TODO fill with top1 accuracy\n",
        "    global_pruning[prune_rate]['top1_acc_rel'] = pruned_acc/orig_acc*100 # Percent accuracy compared to original model\n",
        "    print(f'Top1 accuracy for prune amount {prune_rate}%: {global_pruning[prune_rate][\"top1_acc\"]}%')\n",
        "    print(f'Top1 accuracy (rel) for prune amount {prune_rate}%: {global_pruning[prune_rate][\"top1_acc_rel\"]}%')\n",
        "\n",
        "# TODO plot the results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL_122q4ZZkL",
        "outputId": "acf1e439-d7bd-485c-c17b-12eee27011e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200] 85.5000%\n",
            "[400] 81.7500%\n",
            "[600] 84.6667%\n",
            "[800] 86.8750%\n",
            "[1000] 87.3000%\n",
            "Validation Finished. Pruned Time: 19.7s. Pruned Accuracy: 87.2%\n",
            "Speedup: 1.0262224924891417x. Accuracy drop: 0.09765625%\n",
            "Top1 accuracy for prune amount 5%: 87.20703125%\n",
            "Top1 accuracy (rel) for prune amount 5%: 99.88814317673378%\n",
            "[200] 85.5000%\n",
            "[400] 82.0000%\n",
            "[600] 84.8333%\n",
            "[800] 87.1250%\n",
            "[1000] 87.5000%\n",
            "Validation Finished. Pruned Time: 22.9s. Pruned Accuracy: 87.4%\n",
            "Speedup: 0.8825931540186771x. Accuracy drop: -0.09765625%\n",
            "Top1 accuracy for prune amount 10%: 87.40234375%\n",
            "Top1 accuracy (rel) for prune amount 10%: 100.11185682326622%\n",
            "[200] 85.5000%\n",
            "[400] 82.0000%\n",
            "[600] 84.8333%\n",
            "[800] 87.1250%\n",
            "[1000] 87.5000%\n",
            "Validation Finished. Pruned Time: 22.7s. Pruned Accuracy: 87.4%\n",
            "Speedup: 0.8903181826007368x. Accuracy drop: -0.09765625%\n",
            "Top1 accuracy for prune amount 15%: 87.40234375%\n",
            "Top1 accuracy (rel) for prune amount 15%: 100.11185682326622%\n",
            "[200] 85.5000%\n",
            "[400] 82.0000%\n",
            "[600] 84.8333%\n",
            "[800] 87.1250%\n",
            "[1000] 87.5000%\n",
            "Validation Finished. Pruned Time: 22.2s. Pruned Accuracy: 87.4%\n",
            "Speedup: 0.9109112886255722x. Accuracy drop: -0.09765625%\n",
            "Top1 accuracy for prune amount 20%: 87.40234375%\n",
            "Top1 accuracy (rel) for prune amount 20%: 100.11185682326622%\n",
            "[200] 85.5000%\n",
            "[400] 81.7500%\n",
            "[600] 84.6667%\n",
            "[800] 87.0000%\n",
            "[1000] 87.4000%\n",
            "Validation Finished. Pruned Time: 20.5s. Pruned Accuracy: 87.3%\n",
            "Speedup: 0.9877883053283004x. Accuracy drop: 0.0%\n",
            "Top1 accuracy for prune amount 25%: 87.3046875%\n",
            "Top1 accuracy (rel) for prune amount 25%: 100.0%\n",
            "[200] 85.5000%\n",
            "[400] 82.0000%\n",
            "[600] 84.8333%\n",
            "[800] 87.1250%\n",
            "[1000] 87.5000%\n",
            "Validation Finished. Pruned Time: 20.5s. Pruned Accuracy: 87.4%\n",
            "Speedup: 0.9868780579131492x. Accuracy drop: -0.09765625%\n",
            "Top1 accuracy for prune amount 30%: 87.40234375%\n",
            "Top1 accuracy (rel) for prune amount 30%: 100.11185682326622%\n",
            "[200] 85.5000%\n",
            "[400] 81.7500%\n",
            "[600] 84.6667%\n",
            "[800] 87.0000%\n",
            "[1000] 87.4000%\n",
            "Validation Finished. Pruned Time: 22.3s. Pruned Accuracy: 87.3%\n",
            "Speedup: 0.9067544269898884x. Accuracy drop: 0.0%\n",
            "Top1 accuracy for prune amount 35%: 87.3046875%\n",
            "Top1 accuracy (rel) for prune amount 35%: 100.0%\n",
            "[200] 85.5000%\n",
            "[400] 81.5000%\n",
            "[600] 84.5000%\n",
            "[800] 87.0000%\n",
            "[1000] 87.4000%\n",
            "Validation Finished. Pruned Time: 22.1s. Pruned Accuracy: 87.3%\n",
            "Speedup: 0.9143699209062199x. Accuracy drop: 0.0%\n",
            "Top1 accuracy for prune amount 40%: 87.3046875%\n",
            "Top1 accuracy (rel) for prune amount 40%: 100.0%\n",
            "[200] 85.5000%\n",
            "[400] 81.2500%\n",
            "[600] 84.3333%\n",
            "[800] 86.8750%\n",
            "[1000] 87.2000%\n",
            "Validation Finished. Pruned Time: 22.1s. Pruned Accuracy: 87.1%\n",
            "Speedup: 0.9142259743075797x. Accuracy drop: 0.1953125%\n",
            "Top1 accuracy for prune amount 45%: 87.109375%\n",
            "Top1 accuracy (rel) for prune amount 45%: 99.77628635346755%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02RDvLcRYoO4"
      },
      "source": [
        "## Question 2: Layer-wise Pruning\n",
        "\n",
        "- Perform pruning using `prune.l1_unstructured` on each layer of pretrained VGG16 using various sparsity targets.\n",
        "- Each layer should be pruned away at 5% increments from 5% to 45% total pruning of the model.\n",
        "- Save a copy of each pruned model in a dictionary.\n",
        "- Plot the top-1 accuracy against weight pruned from 0% to 45% (use the original VGG16 for 0%).\n",
        "- What do you observe? Why does layer-wise pruning perform better/worse than global pruning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx2KOvZhYoO5"
      },
      "source": [
        "orig_model = #TODO: load pretrained model\n",
        "prune_rate_list = range(5, 50, 5) # List of prune rate to test from 5-45 inclusive with step of 5\n",
        "layer_pruning = {} # Dictionary to store layer pruning results\n",
        "for prune_rate in prune_rate_list:\n",
        "    model = #TODO: load pretrained model\n",
        "    convs_to_prune = () #TODO: Add conv layers to prune\n",
        "    linears_to_prune = () #TODO: Add linear layers to prune\n",
        "    #TODO: Prune model\n",
        "    layer_pruning[prune_rate_list] = {} # Dictionary to store accuracy results and model for each prune rate\n",
        "    layer_pruning[prune_rate]['model'] = model # Copy pruned model to dictionary\n",
        "    # TODO Run validation on the pruned model\n",
        "    layer_pruning[prune_rate]['top1_acc'] = None # TODO fill with top1 accuracy\n",
        "    layer_pruning[prune_rate]['top1_acc_rel'] = None # Percent accuracy compared to original model\n",
        "    print(f'Top1 accuracy for prune amount {prune_rate}%: {layer_pruning[prune_rate][\"top1_acc\"]}%')\n",
        "    print(f'Top1 accuracy (rel) for prune amount {prune_rate}%: {layer_pruning[prune_rate][\"top1_acc_rel\"]}%')\n",
        "\n",
        "# TODO plot the results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prune_rate_list = range(5, 50, 5) # List of prune rate to test from 5-45 inclusive with step of 5\n",
        "layer_pruning = {} # Dictionary to store global pruning results\n",
        "for prune_rate in prune_rate_list:\n",
        "    model = orig_model #TODO: load pretrained model\n",
        "    parameter_to_prune = (\n",
        "        (model.features[0], 'weight'), # conv1 of VGG16\n",
        "        (model.features[2], 'weight'),\n",
        "        (model.features[5], 'weight'),\n",
        "        (model.features[7], 'weight'),\n",
        "        (model.features[10], 'weight'),\n",
        "        (model.features[12], 'weight'),\n",
        "        (model.features[14], 'weight'),\n",
        "        (model.features[17], 'weight'),\n",
        "        (model.features[19], 'weight'),\n",
        "        (model.features[21], 'weight'),\n",
        "        (model.features[24], 'weight'),\n",
        "        (model.features[26], 'weight'),\n",
        "        (model.features[28], 'weight'),\n",
        "        (model.classifier[0], 'weight'),\n",
        "        (model.classifier[3], 'weight'),\n",
        "        (model.classifier[6], 'weight')\n",
        "        #TODO: Add more layers to prune\n",
        "    )\n",
        "    #TODO: Prune model\n",
        "    for param, name in parameter_to_prune:\n",
        "      prune.l1_unstructured(param, name=name, amount=prune_rate/100.0)\n",
        "    #TODO: make the pruning permanent to increase speed\n",
        "    for param, name in parameter_to_prune:\n",
        "      prune.remove(param, name)\n",
        "    layer_pruning[prune_rate] = {} # Dictionary to store accuracy results and model for each prune rate\n",
        "    layer_pruning[prune_rate]['model'] = model.state_dict() # Copy pruned model to dictionary\n",
        "    #TODO Run validation on the pruned model\n",
        "    pruned_acc, pruned_time = validate_model(model, val_dataloader, 1024, device = device_to_use)\n",
        "    torch.cuda.clear_cache\n",
        "    print(f'Validation Finished. Pruned Time: {pruned_time:.1f}s. Pruned Accuracy: {pruned_acc:.1f}%')\n",
        "    print(f'Speedup: {orig_time/pruned_time}x. Accuracy drop: {orig_acc-pruned_acc}%')\n",
        "    layer_pruning[prune_rate]['top1_acc'] = pruned_acc # TODO fill with top1 accuracy\n",
        "    layer_pruning[prune_rate]['top1_acc_rel'] = pruned_acc/orig_acc*100 # Percent accuracy compared to original model\n",
        "    print(f'Top1 accuracy for prune amount {prune_rate}%: {layer_pruning[prune_rate][\"top1_acc\"]}%')\n",
        "    print(f'Top1 accuracy (rel) for prune amount {prune_rate}%: {layer_pruning[prune_rate][\"top1_acc_rel\"]}%')\n",
        "\n",
        "# TODO plot the results"
      ],
      "metadata": {
        "id": "9isQkshR7TGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8e2a53-bf5d-4c2e-fe14-cfb4cc6c1e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200] 85.5000%\n",
            "[400] 82.0000%\n",
            "[600] 84.8333%\n",
            "[800] 87.1250%\n",
            "[1000] 87.5000%\n",
            "Validation Finished. Pruned Time: 13.3s. Pruned Accuracy: 87.4%\n",
            "Speedup: 1.4896675240135997x. Accuracy drop: -0.09765625%\n",
            "Top1 accuracy for prune amount 5%: 87.40234375%\n",
            "Top1 accuracy (rel) for prune amount 5%: 100.11185682326622%\n",
            "[200] 86.0000%\n",
            "[400] 82.2500%\n",
            "[600] 85.0000%\n",
            "[800] 87.2500%\n",
            "[1000] 87.7000%\n",
            "Validation Finished. Pruned Time: 13.4s. Pruned Accuracy: 87.6%\n",
            "Speedup: 1.4844749690833683x. Accuracy drop: -0.29296875%\n",
            "Top1 accuracy for prune amount 10%: 87.59765625%\n",
            "Top1 accuracy (rel) for prune amount 10%: 100.33557046979867%\n",
            "[200] 86.0000%\n",
            "[400] 82.2500%\n",
            "[600] 85.0000%\n",
            "[800] 87.3750%\n",
            "[1000] 87.6000%\n",
            "Validation Finished. Pruned Time: 13.7s. Pruned Accuracy: 87.5%\n",
            "Speedup: 1.4479009763522686x. Accuracy drop: -0.1953125%\n",
            "Top1 accuracy for prune amount 15%: 87.5%\n",
            "Top1 accuracy (rel) for prune amount 15%: 100.22371364653245%\n",
            "[200] 86.5000%\n",
            "[400] 82.2500%\n",
            "[600] 85.1667%\n",
            "[800] 87.5000%\n",
            "[1000] 87.8000%\n",
            "Validation Finished. Pruned Time: 13.4s. Pruned Accuracy: 87.7%\n",
            "Speedup: 1.4766716513629972x. Accuracy drop: -0.390625%\n",
            "Top1 accuracy for prune amount 20%: 87.6953125%\n",
            "Top1 accuracy (rel) for prune amount 20%: 100.44742729306489%\n",
            "[200] 85.5000%\n",
            "[400] 81.2500%\n",
            "[600] 84.3333%\n",
            "[800] 86.6250%\n",
            "[1000] 86.8000%\n",
            "Validation Finished. Pruned Time: 13.5s. Pruned Accuracy: 86.7%\n",
            "Speedup: 1.4694521222963406x. Accuracy drop: 0.5859375%\n",
            "Top1 accuracy for prune amount 25%: 86.71875%\n",
            "Top1 accuracy (rel) for prune amount 25%: 99.32885906040269%\n",
            "[200] 86.5000%\n",
            "[400] 82.0000%\n",
            "[600] 84.8333%\n",
            "[800] 87.0000%\n",
            "[1000] 86.8000%\n",
            "Validation Finished. Pruned Time: 13.2s. Pruned Accuracy: 86.7%\n",
            "Speedup: 1.5007878640491599x. Accuracy drop: 0.5859375%\n",
            "Top1 accuracy for prune amount 30%: 86.71875%\n",
            "Top1 accuracy (rel) for prune amount 30%: 99.32885906040269%\n",
            "[200] 85.0000%\n",
            "[400] 81.0000%\n",
            "[600] 83.1667%\n",
            "[800] 85.6250%\n",
            "[1000] 85.7000%\n",
            "Validation Finished. Pruned Time: 14.6s. Pruned Accuracy: 85.6%\n",
            "Speedup: 1.3547736064520617x. Accuracy drop: 1.66015625%\n",
            "Top1 accuracy for prune amount 35%: 85.64453125%\n",
            "Top1 accuracy (rel) for prune amount 35%: 98.09843400447427%\n",
            "[200] 85.0000%\n",
            "[400] 81.7500%\n",
            "[600] 83.6667%\n",
            "[800] 85.7500%\n",
            "[1000] 85.2000%\n",
            "Validation Finished. Pruned Time: 13.1s. Pruned Accuracy: 85.2%\n",
            "Speedup: 1.507669228515624x. Accuracy drop: 2.1484375%\n",
            "Top1 accuracy for prune amount 40%: 85.15625%\n",
            "Top1 accuracy (rel) for prune amount 40%: 97.53914988814317%\n",
            "[200] 81.5000%\n",
            "[400] 78.2500%\n",
            "[600] 81.1667%\n",
            "[800] 83.1250%\n",
            "[1000] 83.2000%\n",
            "Validation Finished. Pruned Time: 13.2s. Pruned Accuracy: 83.2%\n",
            "Speedup: 1.497274913851488x. Accuracy drop: 4.1015625%\n",
            "Top1 accuracy for prune amount 45%: 83.203125%\n",
            "Top1 accuracy (rel) for prune amount 45%: 95.30201342281879%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ft0tPUYoO6"
      },
      "source": [
        "## Question 3: Layer-wise (Input) Channel Pruning\n",
        "\n",
        "- Perform pruning using `prune.ln_structured` on each layer of pretrained VGG16 using various sparsity targets.\n",
        "- Prune along the input channels for conv layers and input dimension for linear layers\n",
        "- Do not prune the first conv layer\n",
        "- Each layer should be pruned away at 5% increments from 5% to 45% total pruning of the model.\n",
        "- Save a copy of each pruned model in a dictionary.\n",
        "- Plot the top-1 accuracy against weight pruned from 0% to 45% (use the original VGG16 for 0%).\n",
        "- What do you observe? Why does layer-wise channel pruning perform better/worse than layer-wise unstructured pruning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e77C-pxZYoO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50bc4dd3-bc5a-4fcb-fb12-454ec293289b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200] 85.0000%\n",
            "[400] 77.2500%\n",
            "[600] 78.8333%\n",
            "[800] 81.3750%\n",
            "[1000] 80.8000%\n",
            "Validation Finished. Pruned Time: 12.6s. Pruned Accuracy: 80.9%\n",
            "Speedup: 1.4980694757066781x. Accuracy drop: 6.4453125%\n",
            "Top1 accuracy for prune amount 5%: 80.859375%\n",
            "Top1 accuracy (rel) for prune amount 5%: 92.61744966442953%\n",
            "[200] 80.0000%\n",
            "[400] 66.0000%\n",
            "[600] 65.3333%\n",
            "[800] 71.0000%\n",
            "[1000] 71.0000%\n",
            "Validation Finished. Pruned Time: 12.7s. Pruned Accuracy: 71.0%\n",
            "Speedup: 1.4889027007959605x. Accuracy drop: 16.30859375%\n",
            "Top1 accuracy for prune amount 10%: 70.99609375%\n",
            "Top1 accuracy (rel) for prune amount 10%: 81.31991051454138%\n",
            "[200] 54.0000%\n",
            "[400] 41.0000%\n",
            "[600] 38.6667%\n",
            "[800] 42.8750%\n",
            "[1000] 43.4000%\n",
            "Validation Finished. Pruned Time: 17.4s. Pruned Accuracy: 43.6%\n",
            "Speedup: 1.0816037757057129x. Accuracy drop: 43.75%\n",
            "Top1 accuracy for prune amount 15%: 43.5546875%\n",
            "Top1 accuracy (rel) for prune amount 15%: 49.88814317673378%\n",
            "[200] 28.5000%\n",
            "[400] 18.7500%\n",
            "[600] 17.6667%\n",
            "[800] 20.0000%\n",
            "[1000] 17.9000%\n",
            "Validation Finished. Pruned Time: 12.5s. Pruned Accuracy: 18.0%\n",
            "Speedup: 1.503717778439716x. Accuracy drop: 69.3359375%\n",
            "Top1 accuracy for prune amount 20%: 17.96875%\n",
            "Top1 accuracy (rel) for prune amount 20%: 20.581655480984338%\n",
            "[200] 1.0000%\n",
            "[400] 1.5000%\n",
            "[600] 2.0000%\n",
            "[800] 3.1250%\n",
            "[1000] 3.1000%\n",
            "Validation Finished. Pruned Time: 12.7s. Pruned Accuracy: 3.1%\n",
            "Speedup: 1.4816138186624412x. Accuracy drop: 84.1796875%\n",
            "Top1 accuracy for prune amount 25%: 3.125%\n",
            "Top1 accuracy (rel) for prune amount 25%: 3.5794183445190155%\n",
            "[200] 1.0000%\n",
            "[400] 1.0000%\n",
            "[600] 0.6667%\n",
            "[800] 0.5000%\n",
            "[1000] 0.4000%\n",
            "Validation Finished. Pruned Time: 13.1s. Pruned Accuracy: 0.4%\n",
            "Speedup: 1.4435858330172409x. Accuracy drop: 86.9140625%\n",
            "Top1 accuracy for prune amount 30%: 0.390625%\n",
            "Top1 accuracy (rel) for prune amount 30%: 0.44742729306487694%\n",
            "[200] 1.0000%\n",
            "[400] 0.5000%\n",
            "[600] 0.3333%\n",
            "[800] 0.2500%\n",
            "[1000] 0.2000%\n",
            "Validation Finished. Pruned Time: 12.8s. Pruned Accuracy: 0.2%\n",
            "Speedup: 1.4758209665698576x. Accuracy drop: 87.109375%\n",
            "Top1 accuracy for prune amount 35%: 0.1953125%\n",
            "Top1 accuracy (rel) for prune amount 35%: 0.22371364653243847%\n",
            "[200] 0.0000%\n",
            "[400] 0.0000%\n",
            "[600] 0.1667%\n",
            "[800] 0.1250%\n",
            "[1000] 0.1000%\n",
            "Validation Finished. Pruned Time: 13.0s. Pruned Accuracy: 0.1%\n",
            "Speedup: 1.4486140220732224x. Accuracy drop: 87.20703125%\n",
            "Top1 accuracy for prune amount 40%: 0.09765625%\n",
            "Top1 accuracy (rel) for prune amount 40%: 0.11185682326621924%\n",
            "[200] 0.0000%\n",
            "[400] 0.0000%\n",
            "[600] 0.0000%\n",
            "[800] 0.0000%\n",
            "[1000] 0.0000%\n",
            "Validation Finished. Pruned Time: 12.7s. Pruned Accuracy: 0.0%\n",
            "Speedup: 1.4851300475922617x. Accuracy drop: 87.3046875%\n",
            "Top1 accuracy for prune amount 45%: 0.0%\n",
            "Top1 accuracy (rel) for prune amount 45%: 0.0%\n"
          ]
        }
      ],
      "source": [
        "# prune_rate_list = range(5, 50, 5) # List of prune rate to test from 5-45 inclusive with step of 5\n",
        "# channel_pruning = {} # Dictionary to store channel pruning results\n",
        "# for prune_rate in prune_rate_list:\n",
        "#     model = orig_model#TODO: load pretrained model\n",
        "#     convs_to_prune = () #TODO: Add conv layers to prune (except the first conv layer)\n",
        "#     linears_to_prune = () #TODO: Add linear layers to prune\n",
        "#     #TODO: Prune model\n",
        "#     channel_pruning[prune_rate] = {} # Dictionary to store accuracy results and model for each prune rate\n",
        "#     channel_pruning[prune_rate]['model'] = model # Copy pruned model to dictionary\n",
        "#     # TODO Run validation on the pruned model\n",
        "#     channel_pruning[prune_rate]['top1_acc'] = None # TODO fill with top1 accuracy\n",
        "#     channel_pruning[prune_rate]['top1_acc_rel'] = None # Percent accuracy compared to original model\n",
        "#     print(f'Top1 accuracy for prune amount {prune_rate}%: {channel_pruning[prune_rate][\"top1_acc\"]}%')\n",
        "#     print(f'Top1 accuracy (rel) for prune amount {prune_rate}%: {channel_pruning[prune_rate][\"top1_acc_rel\"]}%')\n",
        "\n",
        "# TODO plot the results\n",
        "\n",
        "prune_rate_list = range(5, 50, 5) # List of prune rate to test from 5-45 inclusive with step of 5\n",
        "channel_pruning = {} # Dictionary to store global pruning results\n",
        "for prune_rate in prune_rate_list:\n",
        "    model = orig_model #TODO: load pretrained model\n",
        "\n",
        "    convs_to_prune = (\n",
        "        (model.features[0], 'weight'), # conv1 of VGG16\n",
        "        (model.features[2], 'weight'),\n",
        "        (model.features[5], 'weight'),\n",
        "        (model.features[7], 'weight'),\n",
        "        (model.features[10], 'weight'),\n",
        "        (model.features[12], 'weight'),\n",
        "        (model.features[14], 'weight'),\n",
        "        (model.features[17], 'weight'),\n",
        "        (model.features[19], 'weight'),\n",
        "        (model.features[21], 'weight'),\n",
        "        (model.features[24], 'weight'),\n",
        "        (model.features[26], 'weight'),\n",
        "        (model.features[28], 'weight'),\n",
        "    )\n",
        "    linears_to_prune = (\n",
        "        (model.classifier[0], 'weight'),\n",
        "        (model.classifier[3], 'weight'),\n",
        "        (model.classifier[6], 'weight'),\n",
        "    )\n",
        "    #TODO: Prune model\n",
        "    for param, name in convs_to_prune:\n",
        "        prune.ln_structured(param, name=name, amount=prune_rate/100.0, n=2, dim=0)\n",
        "    for param, name in linears_to_prune:\n",
        "        prune.ln_structured(param, name=name, amount=prune_rate/100.0, n=2, dim=1)\n",
        "    #TODO: make the pruning permanent to increase speed\n",
        "    for param, name in convs_to_prune:\n",
        "        prune.remove(param, name)\n",
        "    for param, name in linears_to_prune:\n",
        "        prune.remove(param, name)\n",
        "    channel_pruning[prune_rate] = {} # Dictionary to store accuracy results and model for each prune rate\n",
        "    channel_pruning[prune_rate]['model'] = model.state_dict() # Copy pruned model to dictionary\n",
        "    #TODO Run validation on the pruned model\n",
        "    pruned_acc, pruned_time = validate_model(model, val_dataloader, 1024, device = device_to_use)\n",
        "    print(f'Validation Finished. Pruned Time: {pruned_time:.1f}s. Pruned Accuracy: {pruned_acc:.1f}%')\n",
        "    print(f'Speedup: {orig_time/pruned_time}x. Accuracy drop: {orig_acc-pruned_acc}%')\n",
        "    channel_pruning[prune_rate]['top1_acc'] = pruned_acc # TODO fill with top1 accuracy\n",
        "    channel_pruning[prune_rate]['top1_acc_rel'] = pruned_acc/orig_acc*100 # Percent accuracy compared to original model\n",
        "    print(f'Top1 accuracy for prune amount {prune_rate}%: {channel_pruning[prune_rate][\"top1_acc\"]}%')\n",
        "    print(f'Top1 accuracy (rel) for prune amount {prune_rate}%: {channel_pruning[prune_rate][\"top1_acc_rel\"]}%')\n",
        "\n",
        "# TODO plot the results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLONEW7oYoO6"
      },
      "source": [
        "## Question 4: Layer-wise (Input) Channel Pruning (Continued)\n",
        "\n",
        "In this question, we harden the channel pruning by removing the channels\n",
        "- Collect the number of non-zero input channels for each layer\n",
        "- Instantiate a new model based on the number of non-zero input channels\n",
        "- Remember that the output channels of previous layer should match the input channels for current layer\n",
        "- Pay special attention to the last convolution layer and first linear layer\n",
        "- Copy over the the non-zero weights from the pruned model to the hardened model\n",
        "- Calculate the run time and model size for the original VGG16 and the hardened VGG16s\n",
        "- Plot the **relative** top-1 accuracy, run time and model size from 0% to 45% pruned VGG16\n",
        "- What do you observe? What is the trade-off between accuracy, run time, and model size?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXI5ZUtlYoO7"
      },
      "outputs": [],
      "source": [
        "prune_rate_list = range(5, 50, 5) # List of prune rate to test from 5-45 inclusive with step of 5\n",
        "hardened_pruning = {} # Dictionary to store hardened pruning results\n",
        "for prune_rate in prune_rate_list:\n",
        "    #TODO: Collect the number of non-zero channels for each layer\n",
        "    #TODO: Instantiate a new model based on collected number of non-zero channels\n",
        "    hardened_pruning[prune_rate_list] = {} # Dictionary to store accuracy results and model for each prune rate\n",
        "    hardened_pruning[prune_rate]['model'] = model # Copy original model to dictionary\n",
        "    hardened_pruning[prune_rate]['top1_acc'] = None # TODO just copy from channel pruning\n",
        "    hardened_pruning[prune_rate]['top1_acc_rel'] = None # TODO just copy from channel pruning\n",
        "    hardened_pruning[prune_rate]['run_time'] = None # TODO Collect run time of the hardened model\n",
        "    hardened_pruning[prune_rate]['run_time_rel'] = None # TODO Collect run time (relative) of the hardened model\n",
        "    hardened_pruning[prune_rate]['model_size'] = None # TODO Collect model size of the hardened model\n",
        "    hardened_pruning[prune_rate]['model_size_rel'] = None # TODO Collect model size (relative) of the hardened model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZkOrsgBYoO7"
      },
      "outputs": [],
      "source": [
        "#TODO plot the relative accuracy, relative run time, and relative model size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lEP0VQYoO7"
      },
      "source": [
        "## (Optional) Question 5: Regularization to promote sparsity\n",
        "Using lab 1 as guidance, load a pretrained trained VGG16 model without batch normalization (without `_bn` suffix) below and train it using Tiny ImageNet. The specifications are listed below:\n",
        "\n",
        "* The data transform should be according to VGG16_weights parameter description found [here](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html).\n",
        "* Use a batch size of `32`\n",
        "* Use the default `CrossEntropyLoss` loss function as a criterion\n",
        "* Use stochastic gradient descent with learning rate of `1e-3`, momentum of `0.9`, weight decay of `1e-4`\n",
        "* Use a momentum value of `0.9`\n",
        "* Use a weight decay of `1e-4`\n",
        "* Train for 10 Epochs\n",
        "* Train the model with GPU\n",
        "* save the trained model in a file called `original_net`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3hhX3WJYoO7"
      },
      "outputs": [],
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "  \n",
        "# Unzip raw zip file\n",
        "!unzip -qq 'tiny-imagenet-200.zip'\n",
        "\n",
        "# Define main data directory\n",
        "data_dir = Path('tiny-imagenet-200') # Original images come in shapes of [3,64,64]\n",
        "\n",
        "# Define training and validation data paths\n",
        "train_dir = data_dir / 'train'\n",
        "valid_dir = data_dir / 'val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moR3QHhaYoO8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.datapipes.datapipe import T\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "trainset = datasets.ImageFolder(train_dir, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)  # Batch size of 32\n",
        "criterion = nn.CrossEntropyLoss() # Store loss function in this variable\n",
        "original_net = models.efficientnet_b0()\n",
        "print(original_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DV6jYc4YoO8"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(original_net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4) # Store SGD optimizer in this variable\n",
        "\n",
        "# TODO write reuseable validation and training functions here to reduce work\n",
        "# TODO fill out the above variables\n",
        "# TODO run training\n",
        "# TODO save the trained model to a file called original_net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCf9vkztYoO8"
      },
      "source": [
        "## (Optional) Question 6: Accuracy on TinyImageNet\n",
        "\n",
        "Run validation over the entire validation set and report the top 1 and top 5 accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBnsMGtZYoO8"
      },
      "outputs": [],
      "source": [
        "validset = datasets.ImageFolder(valid_dir, transform=transform)\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=32, num_workers=2)\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in validloader:\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = original_net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSRI9bm9YoO8"
      },
      "source": [
        "## (Optional) Question 7: Iterative Pruning\n",
        "Follow the iterative pruning method described in lecture co-design II. Run three iterations of iterative pruning--training for an entire epoch each iteration. The final target prune rate follows the following equation $final = 1 - (1 - r)^n$. Therefore, ensure to pick the correct iterative prune rate, $r$, to meet the final prune rate target $final$. The final prune rate targets are captured in the `prune_rate_list`. $n$ is the number of iterations.\n",
        "\n",
        "- Compare your accuracies to Q1-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F75Ky9BYoO9"
      },
      "outputs": [],
      "source": [
        "iterative_pruning = {} # Dictionary to store pruned model after retraining and accuracy for each prune rate target\n",
        "iterative_prune_rate = None # iterative prune rate needed to achieve `prune_rate` final target when pruned three times\n",
        "for prune_rate in prune_rate_list:\n",
        "    iterative_pruning[prune_rate] = {} # Dictionary to store accuracy and model for each sparsity value\n",
        "    iterative_pruning[prune_rate]['model'] = copy.deepcopy(original_net) # Copy original model to dictionary\n",
        "    # TODO Run iterative pruning three times\n",
        "    iterative_pruning[prune_rate]['top1_acc'] = None # TODO fill with top1 accuracy\n",
        "    iterative_pruning[prune_rate]['top1_acc_loss'] = None # Percent accuracy loss compared to original model\n",
        "    print(f'Top1 accuracy for sparsity {prune_rate}%: {iterative_pruning[prune_rate][\"top1_acc\"]}%')\n",
        "    print(f'Top1 accuracy loss for sparsity {prune_rate}%: {iterative_pruning[prune_rate][\"top1_acc_loss\"]}%')\n",
        "# TODO save the trained model to a file called iterative_pruned_net"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac27ca66b58b4d9db7ea3408426a5f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac61a95d99ac4592b53f2cc602535455",
              "IPY_MODEL_a885235ea97b4e8ea070f3aa8d7242b6",
              "IPY_MODEL_595f0041c48140d9b4f2ca985d6afda2"
            ],
            "layout": "IPY_MODEL_903b1a73edb443c4ba0d85c0d51b6fc8"
          }
        },
        "ac61a95d99ac4592b53f2cc602535455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72d95cd2a2604b89839024fff046179b",
            "placeholder": "​",
            "style": "IPY_MODEL_1808c50ae4074b37b9af5c89d74fc4e3",
            "value": "100%"
          }
        },
        "a885235ea97b4e8ea070f3aa8d7242b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_161812323f504913a4d350365dcc0b43",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4b95eb0084749979e01eac603d4a22c",
            "value": 553433881
          }
        },
        "595f0041c48140d9b4f2ca985d6afda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c2db2c1cbcd4276a33bf047ec14a3f6",
            "placeholder": "​",
            "style": "IPY_MODEL_7afd7ffbdee041f187c88dd96e2469dd",
            "value": " 528M/528M [00:02&lt;00:00, 230MB/s]"
          }
        },
        "903b1a73edb443c4ba0d85c0d51b6fc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d95cd2a2604b89839024fff046179b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1808c50ae4074b37b9af5c89d74fc4e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "161812323f504913a4d350365dcc0b43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4b95eb0084749979e01eac603d4a22c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c2db2c1cbcd4276a33bf047ec14a3f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7afd7ffbdee041f187c88dd96e2469dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}